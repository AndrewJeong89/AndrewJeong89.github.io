---
layout: post
title: "Project 4: Web Scraping/ Logrithmitic Regression - Data Science Salaries"
date: 2016-10-19
categories: Projects
description: 
image: http://www.rcrwireless.com/wp-content/uploads/2014/06/indeed-logo-1024x422.jpg
image-sm:
---

# Data Science Salaries

## WebScraping & Log Regression 

### Premise

A contracting firm that's rapidly expanding wants to leverage data to win more contracts. The firm offers technology and scientific solutions and wants to be competitive in the hiring market. The principal at this firm thinks the best way to gauge salary amounts is to take a look at what industry factors influence the pay scale for these professionals. 

### Assumptions and Risks

Assumptions and risks related to this data set is the low percentage of job positions with salaries and even then most postings are salries from recruiting agencies. Also, salaries are listed in the form of weekly, monthly, yearly and so on. Since the variance in how salary is listed can be a factor in compensation, only the salaries with yearly salaries were taken (most common salary expression). Another risk associated with the data is the reccurence of similar job postings. For example, one company or recruiting firm can post multiple times for one company with slight differences in job description or job title. Many times the job could possibly be the same position or it could be multiple recruiting agencies promoting a certain company/ job. This can skew the data to generalize the data according to specific positions and companies.

Some assumptions we are making is that the mean for each salary range per job listing will serve as our basis for each salary. For example if the salary is between 80K and 100K, the salary used for analysis will be 90K (the mean of the two). 
       
### Web Scraping - Indeed

Using [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/), a webscraping python library, a request was sent to Indeed.com to scrape content on Indeed. Next a function was created to extract location, job title, company name, salary, and job description and summary for data scientist positions from indeed. Job positions with the keywords "data" and "science/scientist" and position locations in either New York, Chicago, Boston, San Francisco, Los Angeles, Austin, and Atlanta were scraped 1000 max entries per city. 

## Data Muning/ Data Cleaning

After extracting the data, the data needed to be cleaned. First, jobs without salaries were removed as our predictive analysis and modeling of the data requires data on salaries. Also dollar signs($), commas (,), and 'nobr' were removed from the salaries column. Salaries are also paid either yearly, monthly, weekly, and hourly. Since only the yearly salaries are pertinent for analysis and is the most common way of noting salaries, monthly, weekly, and hourly salaries were dropped. In addition, since job positings usually present a range of salaries for each posting, the mean of the highest and lowest number in the range was averaged and was used as the single salary amount for each position.

## Exploratory Data Analysis (EDA)

Before performing predictive analysis on the indeed scraped data, general exploring of the salary data is necessary. From the data gathered, the mean salary for data science positions is $103,939 with a standard deviation for $47,204. The highest salary listed is $250,000 and the lowest is $10,000. Next we looked at how many times a value or word shows up in our data. Not surprisingly, New York showed up the most at 78 times for cities. 

## Data Cleaning/ Munging

Steps in cleaning data: 

 1 Convert money columns from string to float

 2 Checking to see if all zipcodes have length of 5 and changed to integer
       
       Found 1 zipcode value '712-2', turns out to be 51529 (changed)
              
<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/Screen%20Shot%202016-10-13%20at%201.30.38%20PM.png?raw=true' >     

       Found 1 zipcode value outside of Iowa, switched from 56201 to 52601
  
<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/wrong_zip_des_moines.png?raw=true' >

 3 All city values changed to uppercase

 4 Replace NaN by creating a dictionary with "City" as key and filled in corresponding "County" and "County Name" as values of       the dictionary
              Updated remaining NaN values for "County" manually
 
 5 Replace NaN values for "Category Name" using "Item Description" and correlating matching values
       * Manually updated related NaN values in "Category Name"       

## Data Analysis / EDA

In order to start forming decisions on locations to open a new store, it is necessary to evaluate possibilities for further data analysis and perform exploratory data analysis. To do this, the date was separated out into seperate columns of months and year to account for seasonality and a new year. Overall gross profit and gross profit per bottle was calculated to see expected profits by store. Number of transactions per county was also calculated. 

In addition, 2010 demographic data for land area per store and population per store was appended to provide a more robust and detailed analysis of our data. The thinking was that reach per store by populatin and alnd size would have an affect on sales. Correlating median sales in relationship to area per store and population per store resulted in no direct correlation.

To account for seasonality affect and differences between months and years, the data was split into 2015 sales and 2016 sales. After calculating the top 10 counties for gross profit and sales, Polk came out overwhelming at the top for both. This may be attributed to the population in the county and the capital city of Iowa being located in Polk.

Lastly, through the exploration of the data and plotting the data, we noticed two extreme outliers. The outliers skewed the liquor sales data to be overwhelmingly right- skewed. To account for the right skew in our data, in determining average sales to predict our models, both median and mean were used to determine average. 

## Data Mining




### P Value & T- Statistic

By comparing each individual rap songs' time to peak compared to rest of the songs' times to peak, we were able to find the P Value and T- Statistic

The null hypothesis is that there is no difference regardless of genre on how long it takes for a song to peak.

The alternative hypothesis is that there is a difference, specifically with rap music.

The T- test revealed T statistic of 1.90 and a P Value of .0577

Since the p-value is higher than the significance level, assuming an alpha of .05, the alternative hypothesis is rejected and the null hypothesis is true.

TO NOTE:
** It is hard to exactly determine whether to reject null or alternative hypothesis in this particular scenario because the P Value is only very slightly higher than the .05 (difference of .007) **

## Considerations and Follow Up

There are multiple considerations to observe when looking at this data. There is significant risk with scraping the terms data and scientist as this brings in "Certifying Scientist" and "Scientists", unrelated to data science and positions for people trying to become scientists. Therefore, the salary data could be highly skewed as the mean salary is lower because of the lower salaries. Also, as mentioned earlier, some of the risk involved with this data is the lack of sufficient data to analyze further all the salaries. More data would need to be scraped for the analysis to be meaningful. 

If this predictive analysis was to be done again, positions and salaries will be vetted carefully to exclude positions unrelated to data science and those that have to do with general scientists. Furthemore, more data will be included to provide a more thorough and accurate prediction for indicators of high salaries. 

In order to account for this looking at the relationship of mean days to peak with the amount of songs in each genre is crucial.

Below are charts showing the relationship between the days to peak by genre and the amount of songs in each genre.


The plot shows only 4 genres with more than 10 songs in the top 100 billboard and some correlation between amount of songs and average days to peak.

Lastly, it is important to consider proportion of time to peak (days on the chart) and how long the song has been in the top 100 by genre.

## Conclusion:

Using genre to predict time to peak is a mostly inconclusive process and prediction model. Other factors need to be taken into account to form a more accurate picture into which songs reach the peak the fastest and whether or not genre has an affect at all on time a song takes to peak.


[Link to Jupyter Notebook](https://andrewjeong89.github.io/2016/10/12/project-3/)




