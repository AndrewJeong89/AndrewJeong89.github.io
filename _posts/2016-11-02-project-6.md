---
layout: post
title: "IMDB Ratings "
date: 2016-11-02
categories: Projects
description: 
image: https://sandmafia.files.wordpress.com/2015/07/imdb.jpg
image-sm:
---

# IMDB MOVIE RATING PREDICTOR

## Scenario

Examine what factors lead to certain ratings for movies. Given that Netflix does not currently store this type of data, your boss has suggested that you collect ratings and reviews data from IMDB. 

    - Netflix uses random forests and decision trees to predict what types of movies an individual user may like.
    - Using unsupervised learning techniques, they are able to continually update suggestions, listings, and other features of    its user interface.
    - Netflix, however, hasn't focused on collecting data on the top movies of all time, and would like to add some of them to       their offerings based on popularity and other factors.

## Prompt: 

Predict whether a movie is highly rated. 

Collect the data, build a random forest, and examine its feature importances to understand what factors contribute to ratings.

Questions to keep in mind:

    - What factors are the most direct predictors of rating?
    - You can use rating as your target variable. But it's up to you whether to treat it as continuous, binary, or multiclass.

## Risks & Assumptions: 

Gross revenue, if unavailable in the scraped data, was set to 0. 
Some columns (features) were removed for reasons that seemed irrelevant to the analysis (deemed by my opinion/ knowledge)


## Web Scraping - IMDB

By using a library called requests, the top 250 IMDB movies were scraped from http://www.imdb.com/chart/top. Then from the top 250 movies from IMDB, using an online API called OMDb (http://www.omdbapi.com/), a function was created to get details about each of the 250 top movies on IMDB. Seperately, gross revenue was scrapped for each movie using the IMDB ID and put into a seperate dataframe.  

## Data Cleaning/ Munging

After converting the scraped data into a dataframe, some of the data containing "N/A" was replaced with NaN, to easily deal with null values. Next, the column "Released" date was converted to a datetime type. Next the "Runtime" column was cleaned by stripping "min" and converting to an integer. The "Year" column was converted to type integer and the "Metascore" column was converted to a float type. For the column containing "imdbVotes", the commas were stripped and converted to integer types. After these changes, The gross revenue dataframe and the IMDB dataframes were combined.

Next, by iterating through all the columns with NaNs, the number of NaN in each column were found. Metascore had the most NaN's with 81 and Gross (revenue) came in second with 66. The other columns with NaN's were much lower with Awards having 4, Poster having 2, and Language, Rated, Released, all having 1. First, the one NaN in language was the movie, "Sunrise". After doing some research, it was known to be an "English" language movie and was changed as such. Next, the movie "Gangs of Wasseypur" had a NaN for rating and was changed to "UNRATED". The remaining category with one NaN, the "Released" column, was "The Gold Rush". After finding the release date from outside resources, the released date was changed to '1925-06-26'. Since there was too many NaN's in the "Gross" (revenue) column, zeros (0) were filled in for NaN values in the "Gross" column. 

Next, columns were dropped because of varying reasons. The awards column was dropped because of issues of colinearity between ratings and awards won. The poster column was dropped because these were images of the poster and not relevant to the analysis at hand. Metascore was dropped because of colinearity and also the copious amount of NaN values, which would make the analysis difficult. The imdbID column was dropped because it contained ids of the movies only relevant to IMDB. The type column had only one category, movies, and was dropped since we're only dealing with movies. The writer was dropped because there were too many writers per movie and believed to be somewhat irrelevant to the analysis. Lastly, the response column was dropped because of the singularity of the responses all saying "True". 

### Text Vectorization

In order to treat each category or "word" as a feature easily and create columns (dummy variables), CountVectorization from the sklearn library is imported. 

(*It is important to note that before running models that one of the columns need to be dropped every time a column is dummified because of the colinearity of the categories*) 

First, using the count vectorizer, each type of genre was given a column and for each movie a 1 was given if the movie was that particular type of genre and a 0 if it wasn't. Then since the genre music had one of the least amount of movies in its category, it was dropped. This step was repeated for actor, language, country, and director. Just as a column was dropped from genre, the columns, country: luxemborg and language: american sign language, were dropped. For director and actor, since movies could have more than one director or actor, these columns didn't need to be dropped. 

After concating the newly formed columnns with the existing, plot was dropped from the dataframe since plots greatly varied between movies and contains paragraphs of words. IMDBRating was converted to a float and duplicates were then dropped.  

## Data Analysis / EDA



<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/IMDB%20Rating%20Predictor/heatmap.png?raw=true' >



<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/IMDB%20Rating%20Predictor/top_gross_movies.png?raw=true' >

<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/IMDB%20Rating%20Predictor/movie_rating.png?raw=true' >

<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/IMDB%20Rating%20Predictor/movie_per_director.png?raw=true' >

<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/IMDB%20Rating%20Predictor/genre_popularity.png?raw=true' >


## Decision Trees

Since some variables had continuous values and others were discrete values, each column was treated differently. Continous variables were left alone while discrete variables were dummified. These were columns were then concatenated to form the X variable for modeling purposes. The "Survived" column was set as the y with 1 being "survived" and 0 being "not survived". Next a train, test split was done to randomize the training data and testing data. From there, in order to normalize the continous variables and transform the data through scaling, a standard scaler was used to scale the data based on the mean.

Next the logistic regression model was created using sci-kit learn. After running the logistic regression, the model was fitted and used to predict survival. By examining the coefficient values of each feature, the impact each feature has on the model could be determined. In this particular model, the male sex, having 3 siblings or spouses, and passenger class 3 were found to have the highest impact on survival aboard the titanic. A negative coefficient indicated a higher chance of dying while a positive coefficient relayed a higher chance of survival. 

#### Classificaiton Report and Benchmarks

The model was then scored to test the accuracy. The accuracy score came out to .78 and the mean of the cross validation score came out to .77. Next a classficaiton report gave us the precision (TP/(TP + FP), recall (TP/(TP + FN), and f1 scores (mean of precision and recall). The average precision score, recall score, and f1 score all came out to .78. 

#### Confusion Matrix

The confusion matrix shows us the performance of the classfication model showing actual vs predictions and how well the model did. It gave a true positive value of 69, true negative value of 115, false positive value of 24 and false negative value of 27. 


### Precision- Recall

The "average precision" method of scoring optimizes parameters for the area under the precision- recall curve and not accuracy. By gridsearching using this scoring method, the best score comes to .82 with a C of 1.05 and a L1 penalty.

This chart shows the relationship between the precision and recall as recall score increases, precision score will decrease. 

<img src= 'https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/Titanic-Images/precision-recall.png?raw=true'>

## Conclusion:

After running various classfication models such as the logistic regression, gridsearch logistic regression, gridsearch KNN, these different models didn't seem to have much variance in accuracy scores. During the exploratory data analysis process, SQL and SQL Alchemy were the tools used to analyze the data and manipulate it. By looking at coefficient importance, it can be determined in the logistic regression model that the male sex, passengers with 3 siblings our spouses, and passengers in class 3 had the biggest negative impact on survival on the titanic, meaning passengers with these qualities had the highest chance of not surviving. Fare and passengers with one parent/ child or one sibling/ spouse, were more likely to survive. Classification reports and confusion matrices were run to find the scores of the models and to see the total true positives, true negatives, false positives, false negatives. The ROC AUC scores provided insight into the relationship between the true positive and true negative rates. Lastly a Precision- Recall curve was also implemented to check relationship between the precision scores and recall scores of the models. Ultimately, to some extent, through the various classification models and different ways of improving the scores of these models whether it be accuracy, precision, recall, f1, and etc, some insight was gained on a passengers survival probability and which factors were important in the survival decision.


[Link to Jupyter Notebook](https://github.com/AndrewJeong89/GA-DSI/blob/master/projects/projects-weekly/project-06/IMDB%20RATING%20-%20AJ.ipynb)
