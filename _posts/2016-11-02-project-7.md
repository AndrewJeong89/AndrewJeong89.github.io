---
layout: post
title: "Airport Delays"
date: 2017-05-09
categories: Projects
description: Cluster Analysis
image: http://www.khaledibrahem.com/wp-content/uploads/2016/10/o-3.jpg
image-sm:
---

# Airport Delays & Clustering

## Scenario

You've been hired by the FAA as a consultant to analyze the operations of major airports around the country. The FAA wants to cut down on delays nationwide, and the most important part of this task is understanding the characteristics and groupings of airports based on a dataset of departure and operational delays.

     - A certain degree of delay is expected in airport operations, however the 
       FAA is noticing significant delays with certain airports
     - When a flight takes off, it's departure delay is recorded in minutes, 
       as well as operational data relating to this delay
     - At the end of the year, this data is averaged out for each airport. 
       Your datasets have these averaged for a 10 year range between 2004 and 2014
     - Over this 10 year range, some delay times have not improved or have worsened.

## Prompt: 

There are three different datasets related to airport operations. These include a dataset detailing the arrival and departure delays/diversions by airport, a dataset that provides metrics related to arrivals and departures for each airport, and a dataset that details names and characteristics for each airport code.

You will help the FAA:

    - Organize and store their data so that they can easily understand it after your consulting work is done
    - Mine and refine the data to uncover its basic attributes and characteristics.
    - Use your skills with PCA to uncover the core components of operations related to delays.

When you've finished your analysis, the FAA would like a report detailing your findings, with recommendations as to which airports and operational characteristics they should target to decrease delays.

Here are some questions to keep in mind:

    - What operational factors are most directly correlated to delays?
    - Take a look at airports groupings - are there any relationships by region? Size?


## Risks & Assumptions: 


## Importing Data and Creating PostgreSQL Database

First, all three datasets are imported from CSV into the Jupyter Notebook in order to start cleaning, perform analysis, and use unsupervised learning techniques. These datasets will help understand the distribution, characteristics, and components od individual airport operations that lead to delays. After reading in the CSV of the datasets, sqlalchemy was imported to create a postgreSQL database. In the database, tables consisting of the airport flights, airport cancellations, and airport operations were created. Next a seperate table was created of all the tables in one by using SQL and joins. 

## Data Cleaning/ Munging

For analysis purposes using python and pandas, the full combined table from the PostgreSQL database was converted to a dataframe. Next the columns, alias, facility type, key, Airport, airport, year, Boundary data available, and county were dropped. Alias was dropped because it was just an reiteration of the AP Name, but had the full entire name and NaNs. Airport and airport were dropped for similar reasons as the names were repeated but as shortened acronyms. Like the airport columns, year was dropped because there were two of the same year columns when the tables were combined. The column key was dropped because it was just a reference/ index number. Since all the facilities are airports, facility type was dropped. County data was dropped because it was unimportant at the county level where the airport was located and because some county data was unavailable and were replaced with #NAME?. Finally, the boundary data available was removed because it is irrelevant to the clustering analysis.

## Data Analysis / EDA

Before running decision tree models, to get familiar and understand the data better, some exploratory analysis is necessary. 

<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/Airport%20-%20Clustering/arrival%20vs%20departure%20cancellations.png?raw=true' >

<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/Airport%20-%20Clustering/arrival%20can%20vs%20arrival%20div.png?raw=true' >



<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/Airport%20-%20Clustering/dept%20can%20vs%20dept%20div.png?raw=true' >



<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/Airport%20-%20Clustering/heatmap-%20airport.png?raw=true' >


<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/Airport%20-%20Clustering/relationship%20matrix.png?raw=true' >

<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/Airport%20-%20Clustering/map%20of%20locations.png?raw=true' >




## Decision Trees

In order to run the decision tree analysis and predict IMDB ratings, first a standard cutoff point was necessary. To do this, the mean was set as the standard benchmark to predict whether the movie was above or below. The mean of 8.31, while may seem very low in the 8.0 to 9.3 range, is a good benchmark because it is representative of the skew of the movie ratings as they generally fall toward the lower spectrum towards 8.0 rather than the 9.3 rating.

Initially by first running a decision tree classifier model, IMDB Votes seems to be 4 times more effective in predicting IMDB rating than the next highest, the year of the movie. The model has a consistent precision, recall, and f1-score of .76. Gridsearching on the decision trees interestingly lowered the precision score to .74. While the recall and f1-scores dropped to .75. Eventually by bagging gridsearch decision trees, the precision score increased to .87 and the recall and f1-scores increased to .85. By bagging and gridsearching decision trees, the scores were significantly increased by about 10%. 

A random forest model and an extra trees model was also run to see if the models would increase precision, recall, or the f1-scores. The random forest model had a precision score of .78 with a recall score of .73 and a f1 score of .69. The extra trees classifier had a precision score of .64, a recall score of .65, and a f1 score of .61. Both models were relatively below the decision tree scores except for the exception of the precision score of the random forest model, which scored higher than the decision tree model and the gridsearched decision trees. Furthermore gridsearching random forests resulted in a lower precision (.66), recall (.67), and f1-score (.62), just as gridsearching decision trees did.

### Feature Importance

<img src= 'https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/IMDB%20Rating%20Predictor/feat_importance_bymodel.png?raw=true'>

When comparing feature importances of the three varying models, decision trees, random forest, and extra trees, imdb votes consistently seems to be the best predictor for IMDB rating. While this may be the case, the impact imdb votes have is significantly higher in the decision tree model than the other two. Runtime seems to be a high factor in feature importance for the random forest and extra trees models, but not for decision trees. The random forest model and the extra tree model seem to be similar in terms of which features are more important for predicting rating. The above chart shows the differing feature importances among the varying models. 

### Accuracy of Models

<img src= 'https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/IMDB%20Rating%20Predictor/model_score%26error.png?raw=true'>

Lastly, when finding the accuracy scores of each of the models, grid searching the random forest model seems to have the highest score. Coming in second is the grid searched bagged decision trees. Interestingly, grid searching decision trees has a lower accuracy score than decisions trees. While very similar in score, grid searching had very little effect on improving the accuracy score of the decision tree model. 

## Conclusion:

The factors that contribute most to the prediction of IMDB ratings are the IMDB votes. Throughout all the models, the more votes that a movie had, the more likely it was to be considered a higher rating. Year and gross revenue were also good indicators of rating for all the models. Runtime, while a high indicator of rating for random forest and extra trees, was not such a good indicator for vanilla decision tree models. 

Not surprisingly, gridsearching random forest models and gridsearching bagged decision trees produced more accurate scores. These differences were negligible and many times it was a difference of at the most 3% and at the least .0001%. gridsearched, gridsearched and bagged, and vanilla decision trees seemed to have the highest margin of error when it came to accuracy scores though. Weirdly enough, extra trees were the lowest in terms of accuracy score and also had a higher erro than both random forest and gridsearched random forest models.

Ultimately, varying models could be more suited for differing types of analysis and there is no one model that is always the best to use. It is important to use and test different models to find the best performing and remembering that there is different ways to score a model is important also. 


[Link to Jupyter Notebook](https://github.com/AndrewJeong89/GA-DSI/blob/master/projects/projects-weekly/project-07/starter-code/project7-%20Airport%20-%20AJ.ipynb)
