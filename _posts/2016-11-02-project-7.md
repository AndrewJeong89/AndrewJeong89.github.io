---
layout: post
title: "Airport Delays"
date: 
categories: Projects
description: Cluster Analysis
image: http://www.khaledibrahem.com/wp-content/uploads/2016/10/o-3.jpg
image-sm:
---

# Airport Delays & Clustering

## Scenario

You've been hired by the FAA as a consultant to analyze the operations of major airports around the country. The FAA wants to cut down on delays nationwide, and the most important part of this task is understanding the characteristics and groupings of airports based on a dataset of departure and operational delays.

    - A certain degree of delay is expected in airport operations, however the FAA is noticing significant delays with certain airports
     - When a flight takes off, it's departure delay is recorded in minutes, as well as operational data relating to this delay
     - At the end of the year, this data is averaged out for each airport. Your datasets have these averaged for a 10 year range between 2004 and 2014
     - Over this 10 year range, some delay times have not improved or have worsened.

## Prompt: 

There are three different datasets related to airport operations. These include a dataset detailing the arrival and departure delays/diversions by airport, a dataset that provides metrics related to arrivals and departures for each airport, and a dataset that details names and characteristics for each airport code.

You will help the FAA:

- Organize and store their data so that they can easily understand it after your consulting work is done
- Mine and refine the data to uncover its basic attributes and characteristics.
- Use your skills with PCA to uncover the core components of operations related to delays.

When you've finished your analysis, the FAA would like a report detailing your findings, with recommendations as to which airports and operational characteristics they should target to decrease delays.

Here are some questions to keep in mind:

- What operational factors are most directly correlated to delays?
- Take a look at airports groupings - are there any relationships by region? Size?


## Risks & Assumptions: 


## Importing Data and Creating PostgreSQL Database

First, all three datasets are imported from CSV into the Jupyter Notebook in order to start cleaning, perform analysis, and use unsupervised learning techniques. These datasets will help understand the distribution, characteristics, and components od individual airport operations that lead to delays. After reading in the CSV of the datasets, 

## Data Cleaning/ Munging

## Data Analysis / EDA

Before running decision tree models, to get familiar and understand the data better, some exploratory analysis is necessary. 

<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/IMDB%20Rating%20Predictor/heatmap.png?raw=true' >

The heatmap shows imdbRatings and imdbVotes have a high positive correlation. The year of the movie and imdbVotes also seem to have a high correlation. The year of the movie and the imdb Rating though has an almost 0 correlation (neither negative or positive). Runtime shows to have very little correlation with any of the other features.

<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/IMDB%20Rating%20Predictor/top_gross_movies.png?raw=true' >

By looking at the top grossing movies in the IMDB top 250, it can be inferred the top five grossing movies are action related. Two Star Wars movies and two Batman movies seem to gross the most. The top 10 highest grossing movies seem to either be from cartoons (Disney), superhero movies, or part of series (Star Wars, Harry Potter, The Lord of the Rings). 

<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/IMDB%20Rating%20Predictor/movie_rating.png?raw=true' >

The bar plot of the movie ratings of the IMDB top 250 movies reveals no movie is rated above a 9.5 (highest being The Shawshank Redemption at 9.3) and in the top 250, the lowest rating is an 8.0. The Godfather and The Godfather: Part II are number 2 and 3 in highest IMDB rating. Cult classics like Pulp Fiction, Shawshank Redemption, The Lord of the Rings, and the The Good, the Bad and the Ugly also rate very highly in the IMDB ratings.

<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/IMDB%20Rating%20Predictor/Screen%20Shot%202017-04-06%20at%201.28.41%20PM.png?raw=true' >

Leonardo DiCaprio, Harrison Ford, and Robert De Niro are tied at 7 for the most appearances in the top 250 IMDB movies. Tom Hanks and Clint Eastwood are a close second. 

<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/IMDB%20Rating%20Predictor/movie_per_director.png?raw=true' >

This plot of the number of movies directed by number of directors shows from the top 250 IMDB movies, most fall in the 2-3 movies range with high IMDB ratings. Surprisingly there are 5 directors who have 5 movies in the top 250 movies. These include famous directors like alfred hitchcock, steven speilberg, stanley kubrick, christopher nolan, and martin scorsese. 

<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/IMDB%20Rating%20Predictor/Screen%20Shot%202017-04-06%20at%201.28.55%20PM.png?raw=true' >

The most popular language in the top 250 highest rated movies on IMDB is english. 211 out of 250 movies or 84% of the top 250 movies are English, which is almost 4 times as popular as the next most popular language, French. There could be a bias on IMDB from the fact that IMDB is a site founded in the UK, an English speaking country, and currently owned by Amazon, a company based in the United States (also English speaking).  

<img src='https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/IMDB%20Rating%20Predictor/genre_popularity.png?raw=true' >

The most popular genre for the top 250 IMDB movies is by far drama. It is important to remember though that movies can have multiple genres and many of these movies contain more than one genre. Genres like adventure, thriller, action, and crime with elements of high activity and suspense were common among the top 250 IMDB films. 


## Decision Trees

In order to run the decision tree analysis and predict IMDB ratings, first a standard cutoff point was necessary. To do this, the mean was set as the standard benchmark to predict whether the movie was above or below. The mean of 8.31, while may seem very low in the 8.0 to 9.3 range, is a good benchmark because it is representative of the skew of the movie ratings as they generally fall toward the lower spectrum towards 8.0 rather than the 9.3 rating.

Initially by first running a decision tree classifier model, IMDB Votes seems to be 4 times more effective in predicting IMDB rating than the next highest, the year of the movie. The model has a consistent precision, recall, and f1-score of .76. Gridsearching on the decision trees interestingly lowered the precision score to .74. While the recall and f1-scores dropped to .75. Eventually by bagging gridsearch decision trees, the precision score increased to .87 and the recall and f1-scores increased to .85. By bagging and gridsearching decision trees, the scores were significantly increased by about 10%. 

A random forest model and an extra trees model was also run to see if the models would increase precision, recall, or the f1-scores. The random forest model had a precision score of .78 with a recall score of .73 and a f1 score of .69. The extra trees classifier had a precision score of .64, a recall score of .65, and a f1 score of .61. Both models were relatively below the decision tree scores except for the exception of the precision score of the random forest model, which scored higher than the decision tree model and the gridsearched decision trees. Furthermore gridsearching random forests resulted in a lower precision (.66), recall (.67), and f1-score (.62), just as gridsearching decision trees did.

### Feature Importance

<img src= 'https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/IMDB%20Rating%20Predictor/feat_importance_bymodel.png?raw=true'>

When comparing feature importances of the three varying models, decision trees, random forest, and extra trees, imdb votes consistently seems to be the best predictor for IMDB rating. While this may be the case, the impact imdb votes have is significantly higher in the decision tree model than the other two. Runtime seems to be a high factor in feature importance for the random forest and extra trees models, but not for decision trees. The random forest model and the extra tree model seem to be similar in terms of which features are more important for predicting rating. The above chart shows the differing feature importances among the varying models. 

### Accuracy of Models

<img src= 'https://github.com/AndrewJeong89/AndrewJeong89.github.io/blob/master/_posts/Images/IMDB%20Rating%20Predictor/model_score%26error.png?raw=true'>

Lastly, when finding the accuracy scores of each of the models, grid searching the random forest model seems to have the highest score. Coming in second is the grid searched bagged decision trees. Interestingly, grid searching decision trees has a lower accuracy score than decisions trees. While very similar in score, grid searching had very little effect on improving the accuracy score of the decision tree model. 

## Conclusion:

The factors that contribute most to the prediction of IMDB ratings are the IMDB votes. Throughout all the models, the more votes that a movie had, the more likely it was to be considered a higher rating. Year and gross revenue were also good indicators of rating for all the models. Runtime, while a high indicator of rating for random forest and extra trees, was not such a good indicator for vanilla decision tree models. 

Not surprisingly, gridsearching random forest models and gridsearching bagged decision trees produced more accurate scores. These differences were negligible and many times it was a difference of at the most 3% and at the least .0001%. gridsearched, gridsearched and bagged, and vanilla decision trees seemed to have the highest margin of error when it came to accuracy scores though. Weirdly enough, extra trees were the lowest in terms of accuracy score and also had a higher erro than both random forest and gridsearched random forest models.

Ultimately, varying models could be more suited for differing types of analysis and there is no one model that is always the best to use. It is important to use and test different models to find the best performing and remembering that there is different ways to score a model is important also. 


[Link to Jupyter Notebook](https://github.com/AndrewJeong89/GA-DSI/blob/master/projects/projects-weekly/project-07/starter-code/project7-%20Airport%20-%20AJ.ipynb)
